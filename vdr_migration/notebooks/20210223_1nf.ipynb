{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import importlib.util\n",
    "import frictionless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20210223\n",
    "\n",
    "The goal of this notebook is to address the data normalization problem with files in [../data/all_stab_scores](../data/all_stab_scores). In short, I want to partition stability score CSV files such that each partition has its own Data Resource and schema. The columns in schema should be as \"close\" to the non-empty columns in the files of the partition as possible. At the same time, we want as few partitions as possible (otherwise, optimal solution would be a single Data Resource for each CSV file).\n",
    "\n",
    "Continuation of [20210222](./20210222.ipynb) notebook.\n",
    "\n",
    "----\n",
    "\n",
    "From today's vimwiki entry:\n",
    "> * We could start with a base case where each node has its own subgraph.\n",
    ">    * Merging subgraphs is defined as taking the union of each subgraph's column set\n",
    ">        * This is a favorable operation if the subgraphs have \"substantially similar\" column sets.\n",
    ">        * \"Substantially similar\" could be defined as length of intersection divided by the length of the union. This metric takes a value between 0 and 1, with 1 being optimal.\n",
    ">        * We could set a threshold for the minimum allowed \"overlap metric\" for a merging operation.\n",
    ">    * The existence of an edge between subgraphs that are not merged is an alternative.\n",
    ">        * This represents a column that would be shared between two subgraphs (tables).\n",
    ">        * An edge with a small weight is the best case, since we could simply add this column to both subgraphs' sets.\n",
    ">        * This operation is optimal when the size of the intersection is minimal compared with the size of the union: in other words, a small \"overlap metric\".\n",
    ">    * We choose either this \"merging two subgraphs\" operation or the \"add small weight edges to both subgraphs\" operation until we reach a state where nodes within a subgraph have high overlap, and there exist no edges between separate subgraphs.\n",
    ">    * An input parameter \"overlap threshold\" would determine at which point a merging operation vs an add-edge operation should be performed. Intuitively, a good starting value is 0.5, though a different value might be optimal.\n",
    ">    * How do we define the optimal solution? In general, we want:\n",
    ">        * Mean Ix(i) / U(s) maximized for each node i in subgraph s\n",
    ">        * Ix / U between subgraphs is, by definition of the termination condition, zero.\n",
    ">    * So this is more of a set problem than a graph problem, but put succinctly:\n",
    ">        * We want to find \"overlap threshold\" s.t. we minimize the mean Intersection / Union(s) for each subgraph s.\n",
    ">    * Unfortunately, it is quite possible that the final solution will contain only one subset that spans the entire set.\n",
    ">* Implementation\n",
    ">    * We start with a list of subsets, one for each file.\n",
    ">        * A subset could be represented as a dict with two items:\n",
    ">            * Which nodes are in the subset\n",
    ">            * The union of all nodes' columns in the subset.\n",
    ">    * For each unique combination of subsets, calculate the overlap metric (p)\n",
    ">        * Given column sets c1 and c2 for a pair of subsets s1 and s2, this is equel to `len(s1.intersection(s2)) / len(s1.union(s2))`\n",
    ">    * If p equals zero for a pair of subsets, ignore this pair.\n",
    ">        * This means that\n",
    ">        * Our work is done for this pair of subsets.\n",
    ">    * Else if p > thresh, then merge the subsets\n",
    ">    * Else, add every column in the intersection to both subsets' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 382\n"
     ]
    }
   ],
   "source": [
    "primary_keys = ['dataset', 'name', 'chip_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fps = glob.glob(\"../data/all_stab_scores/og/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/TASethanho/anaconda3/envs/frictionless/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3427: DtypeWarning: Columns (89) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "def get_cols(fp, primary_keys: list = primary_keys) -> list:\n",
    "    \"\"\"Get columns in CSV file `fp`, and remove primary keys\"\"\"\n",
    "    all_cols = pd.read_csv(fp).columns.tolist()\n",
    "    return [c for c in all_cols if c not in primary_keys]\n",
    "col_dict = {int_idx: get_cols(fp) for int_idx, fp in enumerate(all_fps)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = [{'files': set([int_idx]), 'cols': set(get_cols(fp))} for int_idx, fp in enumerate(all_fps)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over every unique combination of subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_subsets(subs, olap_thresh=0.5, verbose=False):\n",
    "    \"\"\"\"\"\"\n",
    "    s = list(subs)\n",
    "    merge_event = True\n",
    "    n_iters = 0\n",
    "    while merge_event:\n",
    "        # we terminate if no merge event occured this iteration\n",
    "        merge_event = False\n",
    "        \n",
    "        # iterate over every unique combination of subsets\n",
    "        for i in range(len(s)):\n",
    "            for j in range(i + 1, len(s)):\n",
    "                s1 = s[i]\n",
    "                s2 = s[j]\n",
    "                \n",
    "                # skip if one of the subsets was already merged\n",
    "                # (now an empty dictionary)\n",
    "                if not s1 or not s2:\n",
    "                    continue\n",
    "                \n",
    "                # compute ratio of intersection size to union size\n",
    "                olap = len(s1['cols'].intersection(s2['cols'])) / len(s1['cols'].union(s2['cols']))\n",
    "                \n",
    "                if olap == 0:\n",
    "                    # skip if sets are completely distinct\n",
    "                    continue\n",
    "                elif olap > olap_thresh:\n",
    "                    # merge s2 into s1\n",
    "                    s1['files'] = s1['files'].union(s2['files'])\n",
    "                    s1['cols'] = s1['cols'].union(s2['cols'])\n",
    "                    # we replace the merged subset with empty dict\n",
    "                    s[j] = dict()\n",
    "                    merge_event = True\n",
    "        # remove empty dictionaries (merged sets) from the subsets list\n",
    "        s = [sub for sub in s if sub]\n",
    "        n_iters += 1\n",
    "    if verbose:\n",
    "        print(f\"executed {n_iters} merging iterations\")\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = optimal_subsets(subs, 0.4)\n",
    "for s1, s2 in product(*[opt] * 2):\n",
    "    if s1['files'] == s2['files']:\n",
    "        continue\n",
    "    ix = s1['files'].intersection(s2['files'])\n",
    "    assert len(ix) == 0, (s1['files'], s2['files'])\n",
    "len(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, this actually appeared to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A metric we are interested in is the mean similarity between the columns in each file and the columns in the schema (the subset that contains these files). This will essentially represent the proportion of columns in each file that will have non-null values. We can use a modified \"overlap ratio\" for this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group_idx</th>\n",
       "      <th>file_path</th>\n",
       "      <th>n_cols</th>\n",
       "      <th>subset_un</th>\n",
       "      <th>olap</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>file_idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>../data/all_stab_scores/og/topology_mining_rd2...</td>\n",
       "      <td>88</td>\n",
       "      <td>128</td>\n",
       "      <td>0.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>../data/all_stab_scores/og/181114_IL_binders_F...</td>\n",
       "      <td>87</td>\n",
       "      <td>128</td>\n",
       "      <td>0.679688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>../data/all_stab_scores/og/Rocklin_scrambled_c...</td>\n",
       "      <td>88</td>\n",
       "      <td>128</td>\n",
       "      <td>0.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>../data/all_stab_scores/og/Rocklin_display_vec...</td>\n",
       "      <td>88</td>\n",
       "      <td>128</td>\n",
       "      <td>0.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>../data/all_stab_scores/og/Eva1_scrambled_cont...</td>\n",
       "      <td>111</td>\n",
       "      <td>128</td>\n",
       "      <td>0.867188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>../data/all_stab_scores/og/100K_winter19.v3.ex...</td>\n",
       "      <td>112</td>\n",
       "      <td>128</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>../data/all_stab_scores/og/Longxing_scrambled_...</td>\n",
       "      <td>104</td>\n",
       "      <td>128</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>../data/all_stab_scores/og/TwoSix_100K.v4.expe...</td>\n",
       "      <td>115</td>\n",
       "      <td>128</td>\n",
       "      <td>0.898438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>../data/all_stab_scores/og/optE1.v2.experiment...</td>\n",
       "      <td>94</td>\n",
       "      <td>128</td>\n",
       "      <td>0.734375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>../data/all_stab_scores/og/topology_mining_and...</td>\n",
       "      <td>89</td>\n",
       "      <td>128</td>\n",
       "      <td>0.695312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>../data/all_stab_scores/og/optE1_scrambled_con...</td>\n",
       "      <td>90</td>\n",
       "      <td>128</td>\n",
       "      <td>0.703125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>../data/all_stab_scores/og/Eva2_scrambled_cont...</td>\n",
       "      <td>111</td>\n",
       "      <td>128</td>\n",
       "      <td>0.867188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>../data/all_stab_scores/og/Inna.v7.experimenta...</td>\n",
       "      <td>22</td>\n",
       "      <td>44</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>../data/all_stab_scores/og/Rocklin.rd1.v4.expe...</td>\n",
       "      <td>22</td>\n",
       "      <td>44</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>../data/all_stab_scores/og/Eva2.v7.experimenta...</td>\n",
       "      <td>22</td>\n",
       "      <td>44</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>../data/all_stab_scores/og/Rocklin.rd3.v4.expe...</td>\n",
       "      <td>22</td>\n",
       "      <td>44</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>../data/all_stab_scores/og/Rocklin.rd4.v4.expe...</td>\n",
       "      <td>22</td>\n",
       "      <td>44</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>../data/all_stab_scores/og/Rocklin.rd2.v4.expe...</td>\n",
       "      <td>22</td>\n",
       "      <td>44</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>../data/all_stab_scores/og/Eva1.v7.experimenta...</td>\n",
       "      <td>22</td>\n",
       "      <td>44</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>../data/all_stab_scores/og/Rocklin_ssm_stabili...</td>\n",
       "      <td>27</td>\n",
       "      <td>44</td>\n",
       "      <td>0.613636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>../data/all_stab_scores/og/Rocklin.scrambled.e...</td>\n",
       "      <td>22</td>\n",
       "      <td>44</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>../data/all_stab_scores/og/Rocklin.v6.experime...</td>\n",
       "      <td>31</td>\n",
       "      <td>44</td>\n",
       "      <td>0.704545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>../data/all_stab_scores/og/Longxing.v10.experi...</td>\n",
       "      <td>22</td>\n",
       "      <td>44</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>../data/all_stab_scores/og/181114_Benjamin_NTF...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          group_idx                                          file_path  \\\n",
       "file_idx                                                                 \n",
       "0                 0  ../data/all_stab_scores/og/topology_mining_rd2...   \n",
       "1                 0  ../data/all_stab_scores/og/181114_IL_binders_F...   \n",
       "3                 0  ../data/all_stab_scores/og/Rocklin_scrambled_c...   \n",
       "4                 0  ../data/all_stab_scores/og/Rocklin_display_vec...   \n",
       "6                 0  ../data/all_stab_scores/og/Eva1_scrambled_cont...   \n",
       "9                 0  ../data/all_stab_scores/og/100K_winter19.v3.ex...   \n",
       "11                0  ../data/all_stab_scores/og/Longxing_scrambled_...   \n",
       "14                0  ../data/all_stab_scores/og/TwoSix_100K.v4.expe...   \n",
       "16                0  ../data/all_stab_scores/og/optE1.v2.experiment...   \n",
       "17                0  ../data/all_stab_scores/og/topology_mining_and...   \n",
       "18                0  ../data/all_stab_scores/og/optE1_scrambled_con...   \n",
       "22                0  ../data/all_stab_scores/og/Eva2_scrambled_cont...   \n",
       "2                 1  ../data/all_stab_scores/og/Inna.v7.experimenta...   \n",
       "5                 1  ../data/all_stab_scores/og/Rocklin.rd1.v4.expe...   \n",
       "7                 1  ../data/all_stab_scores/og/Eva2.v7.experimenta...   \n",
       "8                 1  ../data/all_stab_scores/og/Rocklin.rd3.v4.expe...   \n",
       "10                1  ../data/all_stab_scores/og/Rocklin.rd4.v4.expe...   \n",
       "12                1  ../data/all_stab_scores/og/Rocklin.rd2.v4.expe...   \n",
       "13                1  ../data/all_stab_scores/og/Eva1.v7.experimenta...   \n",
       "15                1  ../data/all_stab_scores/og/Rocklin_ssm_stabili...   \n",
       "20                1  ../data/all_stab_scores/og/Rocklin.scrambled.e...   \n",
       "21                1  ../data/all_stab_scores/og/Rocklin.v6.experime...   \n",
       "23                1  ../data/all_stab_scores/og/Longxing.v10.experi...   \n",
       "19                2  ../data/all_stab_scores/og/181114_Benjamin_NTF...   \n",
       "\n",
       "          n_cols  subset_un      olap  \n",
       "file_idx                               \n",
       "0             88        128  0.687500  \n",
       "1             87        128  0.679688  \n",
       "3             88        128  0.687500  \n",
       "4             88        128  0.687500  \n",
       "6            111        128  0.867188  \n",
       "9            112        128  0.875000  \n",
       "11           104        128  0.812500  \n",
       "14           115        128  0.898438  \n",
       "16            94        128  0.734375  \n",
       "17            89        128  0.695312  \n",
       "18            90        128  0.703125  \n",
       "22           111        128  0.867188  \n",
       "2             22         44  0.500000  \n",
       "5             22         44  0.500000  \n",
       "7             22         44  0.500000  \n",
       "8             22         44  0.500000  \n",
       "10            22         44  0.500000  \n",
       "12            22         44  0.500000  \n",
       "13            22         44  0.500000  \n",
       "15            27         44  0.613636  \n",
       "20            22         44  0.500000  \n",
       "21            31         44  0.704545  \n",
       "23            22         44  0.500000  \n",
       "19             1          1  1.000000  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_df_from_optimal_subs(subs, optimal_subs, all_fps, col_dict) -> pd.DataFrame:\n",
    "    rows = list()\n",
    "    # df = pd.DataFrame(columns=['file_idx', 'group_idx', 'file_path', 'n_cols', 'subset_ix', 'subset_un'])\n",
    "    for group_idx, subset in enumerate(optimal_subs):\n",
    "        sub_cols = subset['cols']\n",
    "        for file_idx in subset['files']:\n",
    "            cols = set(col_dict[file_idx])\n",
    "            rows.append({\n",
    "                'file_idx': file_idx,\n",
    "                'group_idx': group_idx,\n",
    "                'file_path': all_fps[file_idx],\n",
    "                'n_cols': len(cols),\n",
    "                # by definition, equivalent to n_cols\n",
    "                # 'subset_ix': len(cols.intersection(sub_cols)),\n",
    "                'subset_un': len(cols.union(sub_cols)),\n",
    "            })\n",
    "    as_df = pd.DataFrame(rows)\n",
    "    as_df.set_index('file_idx', inplace=True)\n",
    "    as_df['olap'] = as_df.n_cols / as_df.subset_un\n",
    "    return as_df\n",
    "\n",
    "opt_df = get_df_from_optimal_subs(subs, opt, all_fps, col_dict)\n",
    "opt_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, this worked pretty well. It looks like the algorithm separated the older Rocklin datasets from the more recent ones. The lone `Benjamin` file in group 2 should probably be merged to group 0 (I suspect this is just a column naming issue).\n",
    "\n",
    "Not a column naming issue, just because this file only has one data column `stabilityscore`. In general, the algorithm returns the desired result for cases like these, but we'll manually assign it as group 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_df.loc[19, 'group_idx'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's look at the distribution of overlap ratio for each group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<AxesSubplot:title={'center':'0'}>,\n",
       "       <AxesSubplot:title={'center':'1'}>], dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt_df.hist(column='olap', by='group_idx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the overlap metric is over 0.5 for all data files! With the exception of the file we just manually moved.\n",
    "\n",
    "# Normalize and Write Tables\n",
    "\n",
    "----\n",
    "\n",
    "Use the `normalize_csv` utilities to write normalized CSVs for each group. First, load the `normalize_csv` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = importlib.util.spec_from_file_location(\"module.name\", \"../scripts/normalize_csv.py\")\n",
    "normalize_csv = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(normalize_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lightweight wrapper around `normalize_csv.main`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_group(norm_group, df, out_dir):\n",
    "    print(f\"Normalizing files in group '{norm_group}'\")\n",
    "    files = df['file_path'][df.group_idx == norm_group].tolist()\n",
    "    normalize_csv.main(files=files, out_dir=out_dir, overwrite=True, sort_cols=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize each group separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing files in group '1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/TASethanho/anaconda3/envs/frictionless/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3427: DtypeWarning: Columns (89) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "WARNING:root:Inserted 128 empty columns into the DataFrame read from '../data/all_stab_scores/og/181114_Benjamin_NTF2.experimental_stability_scores.csv'. This accounts for 97.71% of columns in this table.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing files in group '1'\n"
     ]
    }
   ],
   "source": [
    "normalize_group(0, opt_df, '../data/stab_scores')\n",
    "normalize_group(1, opt_df, '../data/stab_scores_legacy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacky check for column number below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/TASethanho/anaconda3/envs/frictionless/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3155: DtypeWarning: Columns (127) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "for fp in glob.glob(os.path.join('../data/stab_scores', \"*_normalized.csv\")):\n",
    "    df = pd.read_csv(fp)\n",
    "    expected_ncols = opt_df[opt_df.group_idx == 1]['subset_un'].iloc[0] + 2\n",
    "    # assert len(df.columns) == expected_ncols, (len(df.columns), expected_ncols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not opt_df.duplicated('file_path').any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "To generate schemas for each group, I used the Frictionless CLI to infer a schema for a candidate file in group 0:\n",
    "```bash\n",
    "$ frictionless describe --json data/stab_scores/100K_winter19.v3.experimental_stability_scores_normalized.csv | jq -r .schema\n",
    "\n",
    "# likewise for group 1\n",
    "$ frictionless describe --json data/stab_scores/Rocklin.v6.experimental_stability_scores_normalized.csv | jq -r .schema\n",
    "```\n",
    "\n",
    "Note that the inferred schema **should** be the same for every file in a given group.\n",
    "\n",
    "Below if a brief reminder of how to describe data using the Python API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file ../data/stab_scores/Rocklin_ssm_stability_scores.experimental_stability_scores_normalized.csv has 46 fields in the schema\n",
      "file ../data/stab_scores/Rocklin.rd2.v4.experimental_stability_scores_normalized.csv has 46 fields in the schema\n",
      "file ../data/stab_scores/optE1_scrambled_controls.experimental_stability_scores_normalized.csv has 131 fields in the schema\n",
      "file ../data/stab_scores/topology_mining_rd2.experimental_stability_scores_normalized.csv has 131 fields in the schema\n",
      "file ../data/stab_scores/Rocklin.v6.experimental_stability_scores_normalized.csv has 46 fields in the schema\n",
      "file ../data/stab_scores/Rocklin.rd3.v4.experimental_stability_scores_normalized.csv has 46 fields in the schema\n",
      "file ../data/stab_scores/Eva2_scrambled_controls.v3.experimental_stability_scores_normalized.csv has 131 fields in the schema\n",
      "file ../data/stab_scores/TwoSix_100K.v4.experimental_stability_scores_normalized.csv has 131 fields in the schema\n",
      "file ../data/stab_scores/Longxing_scrambled_controls.v3.experimental_stability_scores_normalized.csv has 131 fields in the schema\n",
      "file ../data/stab_scores/Rocklin.rd4.v4.experimental_stability_scores_normalized.csv has 46 fields in the schema\n",
      "file ../data/stab_scores/Rocklin.rd1.v4.experimental_stability_scores_normalized.csv has 46 fields in the schema\n",
      "file ../data/stab_scores/Rocklin_display_vector_2.rd4.experimental_stability_scores_normalized.csv has 131 fields in the schema\n",
      "file ../data/stab_scores/Rocklin_scrambled_controls_display_vector_2.rd4.experimental_stability_scores_normalized.csv has 131 fields in the schema\n",
      "file ../data/stab_scores/Eva2.v7.experimental_stability_scores_normalized.csv has 46 fields in the schema\n",
      "file ../data/stab_scores/Inna.v7.experimental_stability_scores_normalized.csv has 46 fields in the schema\n",
      "file ../data/stab_scores/100K_winter19.v3.experimental_stability_scores_normalized.csv has 131 fields in the schema\n",
      "file ../data/stab_scores/181114_Benjamin_NTF2.experimental_stability_scores_normalized.csv has 131 fields in the schema\n",
      "file ../data/stab_scores/optE1.v2.experimental_stability_scores_normalized.csv has 131 fields in the schema\n",
      "file ../data/stab_scores/Eva1.v7.experimental_stability_scores_normalized.csv has 46 fields in the schema\n",
      "file ../data/stab_scores/Rocklin.scrambled.experimental_stability_scores.v2_normalized.csv has 46 fields in the schema\n",
      "file ../data/stab_scores/topology_mining_and_Longxing_all_chips.experimental_stability_scores_normalized.csv has 131 fields in the schema\n",
      "file ../data/stab_scores/181114_IL_binders_FS.v2.experimental_stability_scores_normalized.csv has 131 fields in the schema\n",
      "file ../data/stab_scores/Eva1_scrambled_controls.v3.experimental_stability_scores_normalized.csv has 131 fields in the schema\n",
      "file ../data/stab_scores/Longxing.v10.experimental_stability_scores_normalized.csv has 46 fields in the schema\n"
     ]
    }
   ],
   "source": [
    "for fp in glob.glob(os.path.join('../data/stab_scores', \"*_normalized.csv\")):\n",
    "    fields = frictionless.describe(fp)['schema']['fields']\n",
    "    print(f\"file {fp} has {len(fields)} fields in the schema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the normalized CSVs, such as `data/stab_scores/100K_winter19.v3.experimental_stability_scores_normalized.csv`, surpassed GitHub's 100MB maximum file size. I halved these large files using vim, though a Python utility could easily do this in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/all_stab_scores/og/Inna.v7.experimental_stability_scores.csv',\n",
       " '../data/all_stab_scores/og/Rocklin.rd1.v4.experimental_stability_scores.csv',\n",
       " '../data/all_stab_scores/og/Eva2.v7.experimental_stability_scores.csv',\n",
       " '../data/all_stab_scores/og/Rocklin.rd3.v4.experimental_stability_scores.csv',\n",
       " '../data/all_stab_scores/og/Rocklin.rd4.v4.experimental_stability_scores.csv',\n",
       " '../data/all_stab_scores/og/Rocklin.rd2.v4.experimental_stability_scores.csv',\n",
       " '../data/all_stab_scores/og/Eva1.v7.experimental_stability_scores.csv',\n",
       " '../data/all_stab_scores/og/Rocklin_ssm_stability_scores.experimental_stability_scores.csv',\n",
       " '../data/all_stab_scores/og/Rocklin.scrambled.experimental_stability_scores.v2.csv',\n",
       " '../data/all_stab_scores/og/Rocklin.v6.experimental_stability_scores.csv',\n",
       " '../data/all_stab_scores/og/Longxing.v10.experimental_stability_scores.csv']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_df[opt_df['group_idx'] == 1]['file_path'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
